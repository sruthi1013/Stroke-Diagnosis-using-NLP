{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a dataframe for each of the subreddits, using text extracted from 50 posts, and add a 'comments' column with comments corresponding to each post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request \"submission\" data from subreddit \"stroke\" using the pushshift.io API, and store each post in a dataframe.\n",
    "params = {\n",
    "            \"subreddit\" : \"stroke\",\n",
    "            \"size\" : 50,\n",
    "            \"is_video\" : \"false\"\n",
    "        }\n",
    "\n",
    "base_url = 'https://api.pushshift.io/reddit/submission/search/?'\n",
    "res = requests.get(base_url,params)\n",
    "\n",
    "if res.status_code != 200:\n",
    "    print(f'Error Code: {res.status_code}')\n",
    "else:\n",
    "    stroke = pd.DataFrame(res.json()['data'])\n",
    "    stroke.drop(stroke[stroke['selftext'].str.len() < 2].index, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request \"submission\" data from subreddit \"migraine\" using the pushshift.io API, and store each post in a dataframe.\n",
    "params = {\n",
    "            \"subreddit\" : \"migraine\",\n",
    "            \"size\" : 50,\n",
    "            \"is_video\" : \"false\"\n",
    "        }\n",
    "\n",
    "base_url = 'https://api.pushshift.io/reddit/submission/search/?'\n",
    "res = requests.get(base_url,params)\n",
    "\n",
    "if res.status_code != 200:\n",
    "    print(f'Error Code: {res.status_code}')\n",
    "else:\n",
    "    migraine = pd.DataFrame(res.json()['data'])\n",
    "    migraine.drop(migraine[migraine['selftext'].str.len() < 2].index, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request \"comment\" data from subreddit \"stroke\" using the pushshift.io API, and store each comment\n",
    "# in a separate column(in the 'stroke' dataframe) corresponding to the submission(row in dataframe) it's from.\n",
    "params = {\n",
    "            \"subreddit\" : \"stroke\",\n",
    "            \"size\" : 50\n",
    "            }\n",
    "base_url = 'https://api.pushshift.io/reddit/comment/search/?'\n",
    "res = requests.get(base_url,params)\n",
    "\n",
    "if res.status_code != 200:\n",
    "    print(f'Error Code: {res.status_code}')\n",
    "else:\n",
    "    stroke['comments'] = ''\n",
    "    for j in range(len(res.json()['data'])):\n",
    "        if len(res.json()['data'][j]['body']) > 2:\n",
    "            stroke.loc[stroke['id'] == res.json()['data'][j]['parent_id'][3:], 'comments'] += res.json()['data'][j]['body']\n",
    "            \n",
    "utc_stroke_com = res.json()['data'][49]['created_utc']            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request \"comment\" data from subreddit \"migraine\" using the pushshift.io API, and store each comment\n",
    "# in a separate column(in the 'migraine' dataframe) corresponding to the submission(row in dataframe) it's from.\n",
    "params = {\n",
    "            \"subreddit\" : \"migraine\",\n",
    "            \"size\" : 50\n",
    "            }\n",
    "base_url = 'https://api.pushshift.io/reddit/comment/search/?'\n",
    "res = requests.get(base_url,params)\n",
    "\n",
    "if res.status_code != 200:\n",
    "    print(f'Error Code: {res.status_code}')\n",
    "else:\n",
    "    migraine['comments'] = ''\n",
    "    for j in range(len(res.json()['data'])):\n",
    "        if len(res.json()['data'][j]['body']) > 2:\n",
    "            migraine.loc[migraine['id'] == res.json()['data'][j]['parent_id'][3:], 'comments'] += res.json()['data'][j]['body']\n",
    "            \n",
    "utc_migraine_com = res.json()['data'][49]['created_utc'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to get 5000 posts in reverse chronological order from a subreddit with a 30s delay after every 50 requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_submissions(subreddit, df):  \n",
    "    \n",
    "    for i in range(100):\n",
    "    \n",
    "        date = min(df['created_utc'])\n",
    "\n",
    "        params = {\n",
    "        \"subreddit\" : subreddit,\n",
    "        \"size\" : 50,\n",
    "        \"before\" : date,\n",
    "        \"is_video\" : \"false\"\n",
    "        }\n",
    "        base_url = 'https://api.pushshift.io/reddit/submission/search/?'\n",
    "        res = requests.get(base_url,params)\n",
    "\n",
    "        if res.status_code != 200:\n",
    "            print(f'Error Code: {res.status_code}')\n",
    "        else:\n",
    "            d_f = pd.DataFrame(res.json()['data'])\n",
    "            d_f.drop(d_f[d_f['selftext'].str.len() < 2].index, axis=0, inplace=True)\n",
    "            \n",
    "            df = pd.concat([df, d_f], sort=True)\n",
    "        \n",
    "        time.sleep(30)\n",
    "    \n",
    "    df.reset_index(drop=True, inplace=True)    \n",
    "        \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commenting the below requests so that all the data is not collected again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stroke = get_submissions('stroke', stroke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#migraine = get_submissions('migraine', migraine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to get 5000 comments in reverse chronological order from a subreddit with a 30s delay after every 50 requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(subreddit, df, date):\n",
    "    \n",
    "    if df is stroke:\n",
    "            date = utc_stroke_com\n",
    "    elif df is migraine:\n",
    "            date = utc_migraine_com\n",
    "    \n",
    "    for i in range(100):\n",
    "            \n",
    "        params = {\n",
    "                \"subreddit\" : subreddit,\n",
    "                \"size\" : 50,\n",
    "                \"before\" : date,\n",
    "            }\n",
    "        base_url = 'https://api.pushshift.io/reddit/comment/search/?'\n",
    "\n",
    "        res = requests.get(base_url,params)\n",
    "\n",
    "        if res.status_code != 200:\n",
    "            print(f'Error Code: {res.status_code}')\n",
    "        else:\n",
    "            for j in range(len(res.json()['data'])):\n",
    "                if len(res.json()['data'][j]['body']) > 2:\n",
    "                    df.loc[df['id'] == res.json()['data'][j]['parent_id'][3:],'comments'] += res.json()['data'][j]['body']\n",
    "        \n",
    "        date = res.json()['data'][49]['created_utc']\n",
    "                \n",
    "        time.sleep(30)\n",
    "        \n",
    "    return df, date\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commenting the below requests so that all the data is not collected again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stroke, utc_stroke_com = get_comments('stroke', stroke, utc_stroke_com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#migraine, utc_migraine_com = get_comments('migraine', migraine, utc_migraine_com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing all the posts which have been deleted or removed from each subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke.drop(stroke[stroke['selftext'] == '[deleted]'].index, axis=0, inplace=True)\n",
    "stroke.drop(stroke[stroke['selftext'] == '[removed]'].index, axis=0, inplace=True)\n",
    "stroke.reset_index(drop=True, inplace=True) \n",
    "\n",
    "migraine.drop(migraine[migraine['selftext'] == '[deleted]'].index, axis=0, inplace=True)\n",
    "migraine.drop(migraine[migraine['selftext'] == '[removed]'].index, axis=0, inplace=True)\n",
    "migraine.reset_index(drop=True, inplace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating dataframes for each subreddit only with the extracted text, and adding labels to mark the subreddit it is from, then concatenating both the dataframes and downloading the data into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stroke = stroke[['title', 'selftext', 'comments']]\n",
    "df_migraine = migraine[['title', 'selftext', 'comments']]\n",
    "\n",
    "df_stroke.loc[:,'label'] = 1\n",
    "df_migraine.loc[:,'label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_stroke, df_migraine])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace({'[deleted]':' ','[removed]':' '}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics of the length of the posts and comments from each subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pd.DataFrame(columns = ['stroke', 'migraine'],\n",
    "                     index = ['title_mean','title_median','text_mean','text_median','comment_mean','comment_median'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats['stroke'] = [df_stroke['title'].str.len().mean(),\n",
    "                  df_stroke['title'].str.len().median(),\n",
    "                  df_stroke['selftext'].str.len().mean(),\n",
    "                  df_stroke['selftext'].str.len().median(),\n",
    "                  df_stroke['comments'].str.len().mean(),\n",
    "                  df_stroke['comments'].str.len().median()]\n",
    "\n",
    "stats['migraine'] = [df_migraine['title'].str.len().mean(),\n",
    "                    df_migraine['title'].str.len().median(),\n",
    "                    df_migraine['selftext'].str.len().mean(),\n",
    "                    df_migraine['selftext'].str.len().median(),\n",
    "                    df_migraine['comments'].str.len().mean(),\n",
    "                    df_migraine['comments'].str.len().median()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stroke</th>\n",
       "      <th>migraine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>title_mean</th>\n",
       "      <td>44.068367</td>\n",
       "      <td>40.625619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title_median</th>\n",
       "      <td>36.000000</td>\n",
       "      <td>33.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_mean</th>\n",
       "      <td>1068.691914</td>\n",
       "      <td>688.842887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_median</th>\n",
       "      <td>731.000000</td>\n",
       "      <td>504.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comment_mean</th>\n",
       "      <td>2648.077551</td>\n",
       "      <td>1514.763388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comment_median</th>\n",
       "      <td>1370.000000</td>\n",
       "      <td>782.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     stroke     migraine\n",
       "title_mean        44.068367    40.625619\n",
       "title_median      36.000000    33.000000\n",
       "text_mean       1068.691914   688.842887\n",
       "text_median      731.000000   504.000000\n",
       "comment_mean    2648.077551  1514.763388\n",
       "comment_median  1370.000000   782.000000"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
